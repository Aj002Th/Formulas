{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulas 数据集预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些 import 和设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets_no_test 位置设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_no_test_path = './datasets_no_test/'\n",
    "image_to_latex_datasets_path = './image-to-latex/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一些文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "    folder = os.path.exists(path)\n",
    " \n",
    "    if not folder:                   #判断是否存在文件夹如果不存在则创建为文件夹\n",
    "        os.makedirs(path)            #makedirs 创建文件时如果路径不存在会创建这个路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(datasets_no_test_path + \"dev/images_after_filter\")\n",
    "mkdir(datasets_no_test_path + \"dev/labels_after_filter\")\n",
    "mkdir(datasets_no_test_path + \"dev/labels_after_tokenize\")\n",
    "mkdir(datasets_no_test_path + \"train/images_after_filter\")\n",
    "mkdir(datasets_no_test_path + \"train/labels_after_filter\")\n",
    "mkdir(datasets_no_test_path + \"train/labels_after_tokenize\")\n",
    "\n",
    "# image-to-latex 数据集路径\n",
    "mkdir(image_to_latex_datasets_path)\n",
    "mkdir(image_to_latex_datasets_path + \"formula_images/images_train\")\n",
    "mkdir(image_to_latex_datasets_path + \"formula_images/images_val\")\n",
    "mkdir(image_to_latex_datasets_path + \"formula_images/images_test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization，根据词表进⾏分词，并根据词表初步过滤数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依据 no_chinese.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指明路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_label_dirs = ['./datasets_no_test/dev/labels/', './datasets_no_test/train/labels/']\n",
    "output_label_dirs = ['./datasets_no_test/dev/labels_after_tokenize/', './datasets_no_test/train/labels_after_tokenize/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取字符表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_preprocess/vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    vocab = f.read().split()\n",
    "\n",
    "max_token_len = 0\n",
    "for v in vocab:\n",
    "    if len(v) > max_token_len:\n",
    "        # print(len(v))\n",
    "        max_token_len = len(v)\n",
    "# print(max_token_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搜索函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FMM_func(user_dict, sentence):\n",
    "    \"\"\"\n",
    "    正向最大匹配（FMM）\n",
    "    :param user_dict: 词典\n",
    "    :param sentence: 句子\n",
    "    \"\"\"\n",
    "    # 词典中最长词长度\n",
    "    max_len = max([len(item) for item in user_dict])\n",
    "    start = 0\n",
    "    token_list = []\n",
    "    while start != len(sentence):\n",
    "        index = start+max_len\n",
    "        if index>len(sentence):\n",
    "            index = len(sentence)\n",
    "        for i in range(max_len):\n",
    "            if (sentence[start:index] in user_dict) or (len(sentence[start:index])==1):\n",
    "                token_list.append(sentence[start:index])\n",
    "                # print(sentence[start:index], end='/')\n",
    "                start = index\n",
    "                break\n",
    "            index += -1\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_token_list=[]\n",
    "\n",
    "def tokenize(input_label_dir, output_label_dir):\n",
    "    label_name_list = os.listdir(input_label_dir)\n",
    "    index = 1\n",
    "    for label_name in label_name_list:\n",
    "        # print(index, ':')\n",
    "        index += 1\n",
    "        # print(label_name, ':',end='')\n",
    "        label_file_name = input_label_dir + label_name\n",
    "        with open(label_file_name, 'r', encoding='utf-8') as f1:\n",
    "            content = f1.read()\n",
    "\n",
    "        # print(content)\n",
    "\n",
    "        token_list = FMM_func(vocab, content)\n",
    "        token_list = [token_list[i] for i in range(len(token_list)) if token_list[i] != ' '] # 去除空格\n",
    "        # print(token_list)\n",
    "\n",
    "        new_content = ' '.join(token_list)\n",
    "\n",
    "        # print(new_content)\n",
    "        \n",
    "        have_chinese = False\n",
    "\n",
    "        for token in token_list:\n",
    "            if token not in vocab and token not in ['', ' ']:\n",
    "                # print(label_name, ':',end='')\n",
    "                # print(token)\n",
    "                chinese_token_list.append(token)\n",
    "                have_chinese = True\n",
    "\n",
    "        if have_chinese is not True:\n",
    "            # 保存数据\n",
    "            # shutil.copy(label_file_name, output_label_dir + label_name)\n",
    "            with open(output_label_dir + label_name, 'w', encoding='utf-8') as f:\n",
    "                f.write(new_content)\n",
    "        else:\n",
    "            # 不管了\n",
    "            pass\n",
    "            # with open('./data/math_210421/formula_labels_210421_chinese/' + label_name, 'w', encoding='utf-8') as f:\n",
    "                # f.write(new_content)\n",
    "\n",
    "        # if have_chinese is True:\n",
    "        #     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_label_dir, output_label_dir in zip(input_label_dirs, output_label_dirs): \n",
    "    tokenize(input_label_dir, output_label_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存出现的中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_preprocess/chinese_token.txt', 'w', encoding='utf-8') as f:\n",
    "    chinese_token_list = list(set(chinese_token_list))\n",
    "    for chinese_token in chinese_token_list:\n",
    "        f.write(chinese_token + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过滤多行和内容为error mathpix的标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依据 data_filter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指明路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_label_dirs = ['./datasets_no_test/dev/labels_after_tokenize/', './datasets_no_test/train/labels_after_tokenize/']\n",
    "output_label_dirs = ['./datasets_no_test/dev/labels_after_filter/', './datasets_no_test/train/labels_after_filter/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过滤函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(input_label_dir, output_label_dir):\n",
    "    label_name_list = os.listdir(input_label_dir)\n",
    "    for label_name in label_name_list:\n",
    "        label_file_name = input_label_dir + label_name\n",
    "        with open(label_file_name, 'r', encoding='utf-8') as f1:\n",
    "            lines = f1.readlines()\n",
    "        with open(label_file_name, 'r', encoding='utf-8') as f1:\n",
    "            content = f1.read()\n",
    "\n",
    "        # print(lines)\n",
    "        # print(content)\n",
    "\n",
    "        if len(lines) > 1 or 'e r r o r m a t h p i x' in content:\n",
    "            # 多行和错误的数据直接不要了\n",
    "            continue\n",
    "        \n",
    "\n",
    "        # 通过筛选的数据\n",
    "        shutil.copy(label_file_name, output_label_dir + label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_label_dir, output_label_dir in zip(input_label_dirs, output_label_dirs): \n",
    "    filter(input_label_dir, output_label_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对⻬过滤后的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依据 extract_image_according_to_label_list.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指明路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dirs = ['./datasets_no_test/dev/labels_after_filter/', './datasets_no_test/train/labels_after_filter/']\n",
    "image_dirs = ['./datasets_no_test/dev/images/', './datasets_no_test/train/images/']\n",
    "output_dirs = ['./datasets_no_test/dev/images_after_filter/', './datasets_no_test/train/images_after_filter/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(label_dir, image_dir, output_dir):\n",
    "    label_name_list = os.listdir(label_dir)\n",
    "\n",
    "    for i in range(len(label_name_list)):\n",
    "        label_name_list[i] = label_name_list[i][:-4]\n",
    "\n",
    "    # print(label_list)\n",
    "\n",
    "    image_name_list = os.listdir(image_dir)\n",
    "\n",
    "    for image_name in image_name_list:\n",
    "        if image_name[:-4] in label_name_list:\n",
    "            # print(image_name)\n",
    "            shutil.copy(image_dir + image_name, output_dir + image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_dir, image_dir, output_dir in zip(label_dirs, image_dirs, output_dirs):\n",
    "    extract(label_dir, image_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于 img_to_latex 的专属标签处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_label_dir = './datasets_no_test/dev/labels_after_filter/'\n",
    "train_label_dir = './datasets_no_test/train/labels_after_filter/'\n",
    "\n",
    "output_dir = './image-to-latex/data/'\n",
    "output_file = './image-to-latex/data/im2latex_formulas.norm.lst'\n",
    "\n",
    "validate_image_dir = './datasets_no_test/dev/images_after_filter/'\n",
    "train_image_dir = './datasets_no_test/train/images_after_filter/'\n",
    "test_image_dir = './datasets_no_test/test/images/'\n",
    "image_output_dir = './image-to-latex/data/formula_images/'\n",
    "\n",
    "validate_image_name_list = os.listdir(validate_image_dir)\n",
    "validate_label_name_list = os.listdir(validate_label_dir)\n",
    "train_image_name_list = os.listdir(train_image_dir)\n",
    "train_label_name_list = os.listdir(train_label_dir)\n",
    "test_image_name_list = os.listdir(test_image_dir)\n",
    "\n",
    "random.shuffle(validate_label_name_list)\n",
    "random.shuffle(train_label_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90418\r"
     ]
    }
   ],
   "source": [
    "with open(output_file, 'w', encoding='utf-8') as f0:\n",
    "    index = 0\n",
    "\n",
    "    # train\n",
    "    with open(output_dir + 'im2latex_train_filter.lst', 'w', encoding='utf-8') as f1:\n",
    "        i = 0\n",
    "        for train_label_name in train_label_name_list:\n",
    "            i += 1\n",
    "            print(index, end='\\r')\n",
    "            image_name = train_label_name[:-4] + '.png'\n",
    "            if image_name in train_image_name_list:\n",
    "                # f1.write(image_name + ' ' + str(index) + '\\n')\n",
    "                f1.write(str(i) + '.png' + ' ' + str(index) + '\\n')\n",
    "                shutil.copy(train_image_dir + image_name, image_output_dir + 'images_train/' + str(i) + '.png')\n",
    "                with open(train_label_dir + train_label_name, 'r', encoding='utf-8') as f2:\n",
    "                    line = f2.read()\n",
    "                    f0.write(line + '\\n')\n",
    "                index += 1\n",
    "\n",
    "    # dev\n",
    "    with open(output_dir + 'im2latex_validate_filter.lst', 'w', encoding='utf-8') as f1:\n",
    "        i = 0\n",
    "        for val_label_name in validate_label_name_list:\n",
    "            i += 1\n",
    "            print(index, end='\\r')\n",
    "            image_name = val_label_name[:-4] + '.png'\n",
    "            if image_name in validate_image_name_list:\n",
    "                # f1.write(image_name + ' ' + str(index) + '\\n')\n",
    "                f1.write(str(i) + '.png' + ' ' + str(index) + '\\n')\n",
    "                shutil.copy(validate_image_dir + image_name, image_output_dir + 'images_val/' + str(i) + '.png')\n",
    "                with open(validate_label_dir + val_label_name, 'r', encoding='utf-8') as f2:\n",
    "                    line = f2.read()\n",
    "                    f0.write(line + '\\n')\n",
    "                index += 1\n",
    "\n",
    "    # test\n",
    "    with open(output_dir + 'im2latex_test_filter.lst', 'w', encoding='utf-8') as f1:\n",
    "        i = 0\n",
    "        for image_name in test_image_name_list:\n",
    "            i += 1\n",
    "            print(index, end='\\r')\n",
    "            test_label_name = image_name[:-4] + '.txt'\n",
    "            # f1.write(image_name + ' ' + str(index) + '\\n')                \n",
    "            f1.write(str(i) + '.png' + ' ' + str(index) + '\\n')                \n",
    "            shutil.copy(test_image_dir + image_name, image_output_dir + 'images_test/' + str(i) + '.png')\n",
    "            line = \"1\"\n",
    "            f0.write(line + '\\n')\n",
    "            index += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LaTx_OCR_PRO - tensorflow 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input path\n",
    "validate_label_dir = './datasets_no_test/dev/labels_after_filter/'\n",
    "train_label_dir = './datasets_no_test/train/labels_after_filter/'\n",
    "\n",
    "validate_image_dir = './datasets_no_test/dev/images_after_filter/'\n",
    "train_image_dir = './datasets_no_test/train/images_after_filter/'\n",
    "test_image_dir = './datasets_no_test/test/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output path\n",
    "path = os.path.join(os.curdir, 'latex_ocr', 'data', 'hand')\n",
    "dir_train_output = os.path.join(path, 'train')\n",
    "dir_val_output = os.path.join(path, 'val')\n",
    "dir_test_output = os.path.join(path, 'test')\n",
    "\n",
    "def copy(src, dst):\n",
    "    \"\"\" \n",
    "    src: source dir\n",
    "    dst: destination dir\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dst):\n",
    "        os.makedirs(dst)\n",
    "    \n",
    "    files = os.listdir(src)\n",
    "    for file in files:\n",
    "        src_file = os.path.join(src, file)\n",
    "        dst_file = os.path.join(dst, file)\n",
    "        shutil.copy(src_file, dst_file)\n",
    "\n",
    "copy(train_image_dir, os.path.join(dir_train_output, 'images'))\n",
    "copy(train_label_dir, os.path.join(dir_train_output, 'formulas'))\n",
    "copy(validate_image_dir, os.path.join(dir_val_output, 'images'))\n",
    "copy(validate_label_dir, os.path.join(dir_val_output, 'formulas'))\n",
    "copy(test_image_dir, os.path.join(dir_test_output, 'images'))\n",
    "\n",
    "\n",
    "# build match.txt\n",
    "def build_match(dir, withANS = True):\n",
    "    img_dir = os.path.join(dir, 'images')\n",
    "    files = os.listdir(img_dir)\n",
    "    with open(os.path.join(dir, 'match.txt'), 'w') as f:\n",
    "        for file in files:\n",
    "            f.write(f'{file} {file[:-4]}\\n')\n",
    "\n",
    "build_match(dir_train_output)\n",
    "build_match(dir_val_output)\n",
    "# without test_formula, so build an empty formulas.txt\n",
    "with open(os.path.join(dir_test_output, 'formulas.txt'), 'w') as f:\n",
    "    f.write('0 \\n')\n",
    "\n",
    "# copy vocab\n",
    "with open(os.path.join(os.curdir, 'latex_ocr', 'data', 'vocab.txt'), 'w') as f:\n",
    "    for v in vocab:\n",
    "        f.write(f'{v}\\n')\n",
    "\n",
    "# copy test_ids.txt and build test/match\n",
    "with open(os.path.join(os.curdir, 'datasets_no_test', 'test_ids.txt'), 'r') as f:\n",
    "    ids = f.read().split()\n",
    "    with open(os.path.join(path, 'test', 'match.txt'), 'w') as fl:\n",
    "        for id in ids:\n",
    "            fl.write(f'{id}.png 0\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "719d1ba2a56933e3225a4ce5a6b623ad0990912c1a8e177d0a4cf9de383c4ff3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
